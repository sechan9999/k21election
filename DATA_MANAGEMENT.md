# ëŒ€ìš©ëŸ‰ ì„ ê±° ë°ì´í„° ê´€ë¦¬ ê°€ì´ë“œ

## ğŸ“¦ ë¬¸ì œ ìƒí™©

- **ì„¸ì¢…ì‹œ**: 126í˜ì´ì§€ (manageable)
- **ëŒ€ë„ì‹œ**: ìˆ˜ë°±~ìˆ˜ì²œ í˜ì´ì§€ ê°€ëŠ¥
- **ì „êµ­ í†µí•©**: ìˆ˜ì‹­ GB ì´ìƒ
- **Git ì œí•œ**: ë‹¨ì¼ íŒŒì¼ 100MB, ì €ì¥ì†Œ 5GB ê¶Œì¥

## ğŸ¯ í•´ê²° ì „ëµ

### 1. Git LFS (Large File Storage) ì‚¬ìš©

#### 1.1 Git LFS ì„¤ì¹˜

```bash
# Ubuntu/Debian
sudo apt-get install git-lfs

# macOS
brew install git-lfs

# Windows
# https://git-lfs.github.com/ ì—ì„œ ì„¤ì¹˜

# ì´ˆê¸°í™”
git lfs install
```

#### 1.2 LFS íŠ¸ë˜í‚¹ ì„¤ì •

```bash
# .gitattributes íŒŒì¼ ìƒì„±
cat > .gitattributes << 'EOF'
# PDF íŒŒì¼
*.pdf filter=lfs diff=lfs merge=lfs -text

# ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€
*.png filter=lfs diff=lfs merge=lfs -text
*.jpg filter=lfs diff=lfs merge=lfs -text

# ì••ì¶• íŒŒì¼
*.zip filter=lfs diff=lfs merge=lfs -text
*.tar.gz filter=lfs diff=lfs merge=lfs -text

# ë°ì´í„° íŒŒì¼
data/**/* filter=lfs diff=lfs merge=lfs -text
election_data/**/*.pdf filter=lfs diff=lfs merge=lfs -text
EOF

# LFS íŒŒì¼ íŠ¸ë˜í‚¹
git lfs track "*.pdf"
git lfs track "data/**/*"

# ì»¤ë°‹
git add .gitattributes
git commit -m "Add Git LFS configuration"
```

#### 1.3 LFS íŒŒì¼ ê´€ë¦¬

```bash
# ì¶”ì  ì¤‘ì¸ íŒŒì¼ í™•ì¸
git lfs ls-files

# LFS ìƒíƒœ í™•ì¸
git lfs status

# íŠ¹ì • íŒŒì¼ì„ LFSë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
git lfs migrate import --include="*.pdf"

# LFS íŒŒì¼ í‘¸ì‹œ
git push origin main

# LFS íŒŒì¼ í’€
git lfs pull
```

### 2. ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ í™œìš©

#### 2.1 í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì˜µì…˜

**A. AWS S3**
```python
import boto3

# S3 í´ë¼ì´ì–¸íŠ¸
s3 = boto3.client('s3')

# ì—…ë¡œë“œ
s3.upload_file('seoul.pdf', 'election-data-bucket', 'pdfs/seoul.pdf')

# ë‹¤ìš´ë¡œë“œ
s3.download_file('election-data-bucket', 'pdfs/seoul.pdf', 'seoul.pdf')

# URL ìƒì„± (ê³µê°œ ë§í¬)
url = s3.generate_presigned_url(
    'get_object',
    Params={'Bucket': 'election-data-bucket', 'Key': 'pdfs/seoul.pdf'},
    ExpiresIn=3600  # 1ì‹œê°„ ìœ íš¨
)
```

**B. Google Drive**
```python
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

# Google Drive API
service = build('drive', 'v3', credentials=creds)

# ì—…ë¡œë“œ
file_metadata = {'name': 'seoul.pdf'}
media = MediaFileUpload('seoul.pdf', mimetype='application/pdf')
file = service.files().create(
    body=file_metadata,
    media_body=media,
    fields='id'
).execute()

print(f'File ID: {file.get("id")}')
```

**C. Azure Blob Storage**
```python
from azure.storage.blob import BlobServiceClient

# Blob í´ë¼ì´ì–¸íŠ¸
blob_service_client = BlobServiceClient.from_connection_string(conn_str)
container_client = blob_service_client.get_container_client("election-data")

# ì—…ë¡œë“œ
with open("seoul.pdf", "rb") as data:
    blob_client = container_client.upload_blob(name="seoul.pdf", data=data)
```

#### 2.2 ë°ì´í„° ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì‹œìŠ¤í…œ

**íŒŒì¼ êµ¬ì¡°**:
```
project/
â”œâ”€â”€ .git/
â”œâ”€â”€ code/                    # Gitì— í¬í•¨
â”‚   â”œâ”€â”€ ocr_processor.py
â”‚   â””â”€â”€ batch_processor.py
â”œâ”€â”€ data/                    # Gitì—ì„œ ì œì™¸ (.gitignore)
â”‚   â”œâ”€â”€ seoul.pdf           (ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€)
â”‚   â”œâ”€â”€ busan.pdf           (ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data_manifest.json      # Gitì— í¬í•¨ (ë©”íƒ€ë°ì´í„°ë§Œ)
â””â”€â”€ .gitignore
```

**data_manifest.json**:
```json
{
  "cities": [
    {
      "city_name": "ì„œìš¸íŠ¹ë³„ì‹œ",
      "city_code": "seoul",
      "file_name": "seoul.pdf",
      "file_size_mb": 450.5,
      "total_pages": 2500,
      "file_hash": "a1b2c3d4e5f6...",
      "storage": {
        "type": "s3",
        "bucket": "election-data-kr",
        "key": "pdfs/seoul.pdf",
        "download_url": "https://s3.amazonaws.com/election-data-kr/pdfs/seoul.pdf"
      }
    }
  ]
}
```

**ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸**:
```bash
#!/bin/bash
# download_data.sh

# data_manifest.jsonì—ì„œ URL ì½ì–´ì„œ ë‹¤ìš´ë¡œë“œ
python3 << 'PYTHON'
import json
import urllib.request
from pathlib import Path

with open('data_manifest.json') as f:
    manifest = json.load(f)

Path('data').mkdir(exist_ok=True)

for city in manifest['cities']:
    url = city['storage']['download_url']
    filename = f"data/{city['file_name']}"

    print(f"Downloading {city['city_name']}...")
    urllib.request.urlretrieve(url, filename)
    print(f"  âœ“ Saved to {filename}")

print("All files downloaded!")
PYTHON
```

### 3. ë¶„ì‚° ì²˜ë¦¬ ì „ëµ

#### 3.1 ë„ì‹œë³„ ë¶„í•  ì²˜ë¦¬

```bash
# ê° ë„ì‹œë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
python3 batch_processor.py --city seoul --workers 8
python3 batch_processor.py --city busan --workers 8
python3 batch_processor.py --city daegu --workers 8

# ê²°ê³¼ë§Œ Gitì— ì»¤ë°‹ (PDFëŠ” ì œì™¸)
git add election_data/seoul/ocr_results/*.json
git commit -m "Add Seoul OCR results"
```

#### 3.2 í˜ì´ì§€ ë²”ìœ„ë³„ ë¶„í• 

```bash
# ëŒ€ìš©ëŸ‰ PDFë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í•  ì²˜ë¦¬
python3 ocr_processor_multiprocessing.py seoul.pdf \
    --first-page 1 --last-page 500 \
    --output-dir results/seoul_chunk1 &

python3 ocr_processor_multiprocessing.py seoul.pdf \
    --first-page 501 --last-page 1000 \
    --output-dir results/seoul_chunk2 &

python3 ocr_processor_multiprocessing.py seoul.pdf \
    --first-page 1001 --last-page 1500 \
    --output-dir results/seoul_chunk3 &

# ê²°ê³¼ ë³‘í•©
python3 merge_results.py results/seoul_chunk* \
    --output results/seoul_complete
```

#### 3.3 í´ëŸ¬ìŠ¤í„° ì²˜ë¦¬ (ì„ íƒì‚¬í•­)

```python
# Daskë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° ì²˜ë¦¬
from dask.distributed import Client, as_completed
from dask import delayed

client = Client()  # ë¡œì»¬ í´ëŸ¬ìŠ¤í„°

@delayed
def process_page_range(pdf_path, start, end):
    from ocr_processor_multiprocessing import MultiProcessingOCR
    processor = MultiProcessingOCR()
    return processor.process_pdf_parallel(
        pdf_path, first_page=start, last_page=end
    )

# ì‘ì—… ìƒì„±
tasks = []
chunk_size = 100
for i in range(0, 2500, chunk_size):
    task = process_page_range('seoul.pdf', i+1, min(i+chunk_size, 2500))
    tasks.append(task)

# ë³‘ë ¬ ì‹¤í–‰
results = client.compute(tasks)
client.gather(results)
```

### 4. ê²°ê³¼ë¬¼ ì••ì¶• ë° ì•„ì¹´ì´ë¹™

#### 4.1 ì²˜ë¦¬ ê²°ê³¼ ì••ì¶•

```bash
# ë„ì‹œë³„ ê²°ê³¼ ì••ì¶•
cd election_data/seoul/ocr_results
tar -czf ../seoul_ocr_results.tar.gz *.json *.png

# Gitì—ëŠ” ì••ì¶• íŒŒì¼ë§Œ ì»¤ë°‹
cd ../..
git add seoul/seoul_ocr_results.tar.gz
git commit -m "Add Seoul OCR results (compressed)"
```

#### 4.2 ì„ íƒì  ë°ì´í„° ë³´ê´€

```
ë³´ê´€ ìš°ì„ ìˆœìœ„:
1. ì›ë³¸ PDF        â†’ ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ (S3, Drive)
2. OCR JSON ê²°ê³¼   â†’ Git (ì••ì¶•)
3. ì²˜ë¦¬ëœ ì´ë¯¸ì§€    â†’ ì„ íƒì  ë³´ê´€ (ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€)
4. ì„ì‹œ íŒŒì¼       â†’ ì‚­ì œ
```

### 5. .gitignore ì„¤ì •

```bash
cat > .gitignore << 'EOF'
# ëŒ€ìš©ëŸ‰ ë°ì´í„° íŒŒì¼
*.pdf
data/
election_data/**/*.pdf
election_data/**/*.png

# ì„ì‹œ ì²˜ë¦¬ íŒŒì¼
ocr_results/
ocr_results_mp/
pdf_analysis/
temp/
cache/

# Python
__pycache__/
*.pyc
*.pyo
*.egg-info/
.pytest_cache/
venv/
.venv/

# ì‹œìŠ¤í…œ
.DS_Store
Thumbs.db
*.swp
*.swo

# ì„¤ì • íŒŒì¼ (ë¯¼ê°ì •ë³´)
config.local.json
credentials.json
.env

# ë¡œê·¸
*.log
logs/

# ì••ì¶• íŒŒì¼ (ì„ íƒì )
# *.tar.gz
# *.zip
EOF

git add .gitignore
git commit -m "Add comprehensive .gitignore"
```

### 6. ì‹¤ì „ ì›Œí¬í”Œë¡œìš°

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ìƒˆë¡œìš´ ë„ì‹œ ë°ì´í„° ì¶”ê°€

```bash
# 1. PDFë¥¼ ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ì— ì—…ë¡œë“œ
aws s3 cp incheon.pdf s3://election-data-kr/pdfs/incheon.pdf

# 2. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì— ì¶”ê°€
python3 batch_processor.py --add-city \
    --name "ì¸ì²œê´‘ì—­ì‹œ" \
    --code incheon \
    --url "https://s3.amazonaws.com/election-data-kr/pdfs/incheon.pdf"

# 3. ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
python3 download_data.py --city incheon

# 4. OCR ì²˜ë¦¬
python3 ocr_processor_multiprocessing.py data/incheon.pdf \
    --workers 8 \
    --output-dir election_data/incheon/ocr_results

# 5. ê²°ê³¼ ì••ì¶• ë° ì»¤ë°‹
cd election_data/incheon
tar -czf incheon_ocr_results.tar.gz ocr_results/*.json
git add incheon_ocr_results.tar.gz
git commit -m "Add Incheon OCR results"

# 6. ì›ë³¸ PDF ì‚­ì œ (ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ì— ìˆìœ¼ë¯€ë¡œ)
rm ../../data/incheon.pdf

# 7. í‘¸ì‹œ
git push origin main
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ì „êµ­ í†µí•© ì²˜ë¦¬

```bash
# 1. ì„¤ì • ë¡œë“œ
python3 batch_processor.py --load-config

# 2. Dry-runìœ¼ë¡œ ê³„íš í™•ì¸
python3 batch_processor.py --process --dry-run

# 3. ìš°ì„ ìˆœìœ„ ë†’ì€ ë„ì‹œë¶€í„° ì²˜ë¦¬
python3 batch_processor.py --process --priority 1

# 4. ì•¼ê°„ì— ì „ì²´ ì²˜ë¦¬
nohup python3 batch_processor.py --process --all \
    > batch_processing.log 2>&1 &

# 5. ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
tail -f batch_processing.log
```

### 7. ì„±ëŠ¥ ì˜ˆì¸¡

#### 7.1 ì²˜ë¦¬ ì‹œê°„ ì¶”ì •

```python
# ì„¸ì¢…ì‹œ ê¸°ì¤€ (126í˜ì´ì§€)
base_time_per_page = 2  # ì´ˆ (200 DPI, 8 ì›Œì»¤)

def estimate_processing_time(total_pages, num_workers=8):
    """ì²˜ë¦¬ ì‹œê°„ ì˜ˆì¸¡"""
    # ì„¸ì¢…ì‹œ ëŒ€ë¹„ ìŠ¤ì¼€ì¼ë§
    time_per_page = base_time_per_page * (8 / num_workers)
    total_seconds = total_pages * time_per_page

    return {
        'total_seconds': total_seconds,
        'minutes': total_seconds / 60,
        'hours': total_seconds / 3600,
        'pages': total_pages,
        'workers': num_workers
    }

# ì˜ˆì‹œ
cities = {
    'ì„¸ì¢…': 126,
    'ì„œìš¸': 2500,
    'ê²½ê¸°': 5000,
    'ë¶€ì‚°': 1500
}

for city, pages in cities.items():
    est = estimate_processing_time(pages, num_workers=8)
    print(f"{city}: {pages}í˜ì´ì§€ â†’ {est['hours']:.1f}ì‹œê°„")

# ì¶œë ¥:
# ì„¸ì¢…: 126í˜ì´ì§€ â†’ 0.1ì‹œê°„
# ì„œìš¸: 2500í˜ì´ì§€ â†’ 1.4ì‹œê°„
# ê²½ê¸°: 5000í˜ì´ì§€ â†’ 2.8ì‹œê°„
# ë¶€ì‚°: 1500í˜ì´ì§€ â†’ 0.8ì‹œê°„
```

#### 7.2 ìŠ¤í† ë¦¬ì§€ ì˜ˆì¸¡

```python
def estimate_storage(total_pages):
    """ìŠ¤í† ë¦¬ì§€ ìš”êµ¬ëŸ‰ ì˜ˆì¸¡"""
    # í˜ì´ì§€ë‹¹ í‰ê·  í¬ê¸°
    png_per_page_mb = 0.25      # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€
    json_per_page_kb = 5        # OCR ê²°ê³¼ JSON

    total_png_mb = total_pages * png_per_page_mb
    total_json_mb = total_pages * json_per_page_kb / 1024

    return {
        'images_mb': total_png_mb,
        'images_gb': total_png_mb / 1024,
        'json_mb': total_json_mb,
        'total_gb': (total_png_mb + total_json_mb) / 1024
    }

# ì „êµ­ ì¶”ì •
total_pages_nationwide = 50000  # ê°€ì •
storage = estimate_storage(total_pages_nationwide)

print(f"ì „êµ­ 50,000í˜ì´ì§€:")
print(f"  ì´ë¯¸ì§€: {storage['images_gb']:.1f} GB")
print(f"  JSON: {storage['json_mb']:.0f} MB")
print(f"  ì´í•©: {storage['total_gb']:.1f} GB")

# ì¶œë ¥:
# ì „êµ­ 50,000í˜ì´ì§€:
#   ì´ë¯¸ì§€: 12.2 GB
#   JSON: 244 MB
#   ì´í•©: 12.4 GB
```

## ğŸ“Š ê¶Œì¥ ì„¤ì •

### ì†Œê·œëª¨ í”„ë¡œì íŠ¸ (< 10ê°œ ë„ì‹œ)
- **Git**: ì§ì ‘ ì‚¬ìš© + Git LFS
- **ì²˜ë¦¬**: ë¡œì»¬ ë©€í‹°í”„ë¡œì„¸ì‹±
- **ìŠ¤í† ë¦¬ì§€**: Git LFS (PDF) + GitHub (ê²°ê³¼)

### ì¤‘ê·œëª¨ í”„ë¡œì íŠ¸ (10-50ê°œ ë„ì‹œ)
- **Git**: ê²°ê³¼ë§Œ ì»¤ë°‹
- **ì²˜ë¦¬**: ë¡œì»¬ + í´ë¼ìš°ë“œ VM
- **ìŠ¤í† ë¦¬ì§€**: S3 (PDF) + Git (ê²°ê³¼ ì••ì¶•)

### ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ (ì „êµ­)
- **Git**: ì½”ë“œ + ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë§Œ
- **ì²˜ë¦¬**: í´ë¼ìš°ë“œ ë¶„ì‚° ì²˜ë¦¬
- **ìŠ¤í† ë¦¬ì§€**: S3/GCS (ëª¨ë“  ë°ì´í„°)
- **DB**: PostgreSQL/MongoDB (ê²°ê³¼ ì €ì¥)

## ğŸš€ ì‹œì‘í•˜ê¸°

```bash
# 1. Git LFS ì„¤ì •
git lfs install
git lfs track "*.pdf"

# 2. ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì„¤ì •
python3 batch_processor.py --setup

# 3. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
python3 batch_processor.py --manifest

# 4. Dry-run í…ŒìŠ¤íŠ¸
python3 batch_processor.py --process --dry-run

# 5. ì‹¤ì œ ì²˜ë¦¬
python3 batch_processor.py --process
```

---

**ì‘ì„±ì¼**: 2025-11-18
**ë²„ì „**: 1.0
